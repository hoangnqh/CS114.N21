{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers.tokenization_utils_base import TruncationStrategy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "import json\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import underthesea # ThÆ° viá»‡n tÃ¡ch tá»«\n",
    "from pyvi import ViTokenizer, ViPosTagger # thÆ° viá»‡n NLP tiáº¿ng Viá»‡t\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import dump\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Äá»c dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'X_train[0]'\n",
      "'KÃªu 1 cáº·p 2 cÃ¡i Ä‘áº·t x5 mÃ  giao Ä‘c 5 cÃ¡i shop lÃ m Äƒn kiá»ƒu gÃ¬ v'\n"
     ]
    }
   ],
   "source": [
    "# Äá»c dá»¯ liá»‡u tá»« file Excel\n",
    "file_path = \"Book1.xlsx\"\n",
    "data_frame = pd.read_excel(file_path, header=None)\n",
    "\n",
    "# Truy cáº­p vÃ o cá»™t thá»© nháº¥t vÃ  thá»© hai\n",
    "labels = data_frame.iloc[:, 0].tolist()\n",
    "reviews = data_frame.iloc[:, 1].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=20520051, shuffle=True)\n",
    "pprint(\"X_train[0]\")\n",
    "pprint(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ™‚ğŸ‘¿ğŸ˜ ğŸ˜¡ğŸ™ğŸ˜’ğŸ˜«ğŸ˜•ğŸ˜©ğŸ˜£ğŸ˜ŸğŸ˜­ğŸ˜¢ğŸ˜–ğŸ˜”ğŸ˜â˜¹ï¸ğŸ¤§ğŸ¤’ğŸ˜·ğŸ¤•ğŸ˜µğŸ¤¢ğŸ¤ ğŸ¤¡ğŸ‘¹ğŸ‘ºğŸ‘»ğŸ’€ğŸ‘½ğŸ˜°ğŸ˜¨ğŸ˜§ğŸ¤¥ğŸ˜ˆğŸ™ƒğŸ‘ğŸ¤¬ğŸ‘‹ğŸ˜ŒğŸ˜ğŸ˜¬ğŸ’©ğŸ˜“ğŸ˜¤ğŸ˜®â€ğŸ’¨ğŸ˜‘\n",
      "ğŸ˜€ğŸ˜ŠğŸ˜‰ğŸ˜ğŸ˜˜ğŸ˜—ğŸ˜™ğŸ¤—ğŸ˜šğŸ˜›ğŸ˜ğŸ˜œğŸ˜‹ğŸ¤‘ğŸ˜ğŸ˜‡â™¥ï¸â¤ï¸ğŸ’›ğŸ’šğŸ’™ğŸ’œğŸ–¤ğŸ’–ğŸ’ğŸ¤©ğŸ¥°ğŸ˜…ğŸ’ğŸ’‹ğŸ¤—ğŸ˜‹ğŸ¥³\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tichcuc = \"sáº£n pháº©m tá»‘t cháº¥t lÆ°á»£ng tá»‘t \"\n",
    "tieucuc = \"sáº£n pháº©m tá»‡ kÃ©m cháº¥t lÆ°á»£ng \"\n",
    "with open('icon_tieu_cuc.txt', 'r', encoding='utf-8') as file:\n",
    "    icon_tieu_cuc = file.read()\n",
    "print(icon_tieu_cuc)\n",
    "with open('icon_tich_cuc.txt', 'r', encoding='utf-8') as file:\n",
    "    icon_tich_cuc = file.read()\n",
    "print(icon_tich_cuc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HÃ m chuáº©n hoÃ¡ cÃ¢u\n",
    "def standardize_data(row):\n",
    "    # Icon biá»ƒu tÆ°á»£ng\n",
    "    for val in row:\n",
    "        if val in icon_tich_cuc:\n",
    "            row = tichcuc + row\n",
    "        if val in icon_tieu_cuc:\n",
    "            row = tieucuc + row\n",
    "    # XoÃ¡ háº¿t nhá»¯ng cÃ¡i khÃ´ng pháº£i chá»¯ vÃ  sá»‘\n",
    "    row =  re.sub(r\"[^\\w\\s]\", \" \", row)\n",
    "    \n",
    "    # # XÃ³a dáº¥u cháº¥m, pháº©y, há»i á»Ÿ cuá»‘i cÃ¢u\n",
    "    # row = re.sub(r\"[\\.,\\?]+$-\", \"\", row)\n",
    "    # # XÃ³a táº¥t cáº£ dáº¥u cháº¥m, pháº©y, cháº¥m pháº©y, cháº¥m thang, ... trong cÃ¢u\n",
    "    # row = row.replace(\",\", \" \").replace(\".\", \" \") \\\n",
    "    #     .replace(\";\", \" \").replace(\"â€œ\", \" \") \\\n",
    "    #     .replace(\":\", \" \").replace(\"â€\", \" \") \\\n",
    "    #     .replace('\"', \" \").replace(\"'\", \" \") \\\n",
    "    #     .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
    "    #     .replace(\"-\", \" \").replace(\"?\", \" \")\n",
    "    # # Náº¿u cÃ³ nhiá»u khoáº£ng tráº¯ng thÃ¬ rÃºt gá»n cÃ²n 1\n",
    "    row = row.replace('\\n',\" \")\n",
    "    row = re.sub(r\"\\s+\", \" \", row)\n",
    "    # ÄÆ°a háº¿t vá» chá»¯ thÆ°á»ng\n",
    "    row = row.strip().lower()\n",
    "    str = \"\"\n",
    "    str += row[0]\n",
    "    for i in range(1, len(row)):\n",
    "        if row[i] != row[i - 1]:\n",
    "            str += row[i]\n",
    "    # pprint(str)\n",
    "    # print(ascii(row[16]))\n",
    "    # Chuyá»ƒn thÃ nh dáº¡ng khÃ´ng dáº¥u\n",
    "    # str = unidecode(str)\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HÃ m load danh sÃ¡ch cÃ¡c tá»« vÃ´ nghÄ©a: láº¯m, áº¡, Ã , bá»‹, vÃ¬..\n",
    "sw = []\n",
    "with open(\"vietnamese-stopwords - Copy.txt\", encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    sw.append(line.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model_trans = AutoModel.from_pretrained(\"Fsoft-AIC/videberta-xsmall\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Fsoft-AIC/videberta-xsmall\")\n",
    "\n",
    "# model_trans = AutoModel.from_pretrained(\"Fsoft-AIC/videberta-base\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Fsoft-AIC/videberta-base\")\n",
    "\n",
    "# model_trans = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "# model_trans = AutoModel.from_pretrained(\"vinai/phobert-large\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-large\")\n",
    "\n",
    "model_trans = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tiá»n xá»­ lÃ½ dá»¯ liá»‡u sá»­ dá»¥ng PhoBERT   \n",
    "\n",
    "\n",
    "# HÃ m táº¡o ra bert features\n",
    "def make_bert_features(v_text):\n",
    "    v_tokenized = []\n",
    "    max_len = 150 # Má»—i cÃ¢u dÃ i tá»‘i Ä‘a 200 tá»«\n",
    "    for i_text in v_text:\n",
    "        # print(\"Äang xá»­ lÃ½ line = \", i_text)\n",
    "        # Chuáº©n hÃ³a\n",
    "        \n",
    "        i_text = i_text.replace(\"\\n\",\" \")\n",
    "        i_text = standardize_data(i_text)\n",
    "\n",
    "        # # PhÃ¢n thÃ nh tá»«ng tá»«\n",
    "        # line = underthesea.word_tokenize(i_text)\n",
    "\n",
    "        # # Lá»c cÃ¡c tá»« vÃ´ nghÄ©a\n",
    "        # # filtered_sentence = [w for w in line if not w in sw]\n",
    "        # filtered_sentence = line\n",
    "\n",
    "        # # GhÃ©p láº¡i thÃ nh cÃ¢u nhÆ° cÅ© sau khi lá»c\n",
    "        # line = \" \".join(filtered_sentence[:100])\n",
    "        # line = underthesea.word_tokenize(line, format=\"text\")\n",
    "        line = ViTokenizer.tokenize(i_text)[:150]\n",
    "    \n",
    "\n",
    "        # print(\"Word segment  = \", line)\n",
    "        # Tokenize bá»Ÿi BERT\n",
    "        line = tokenizer.encode(line)\n",
    "        v_tokenized.append(line)\n",
    "\n",
    "    padded = []\n",
    "    # ChÃ¨n thÃªm sá»‘ 1 vÃ o cuá»‘i cÃ¢u náº¿u nhÆ° khÃ´ng Ä‘á»§ tá»« hoáº·c xÃ³a náº¿u dÆ°\n",
    "    for line in v_tokenized:\n",
    "        if len(line) < max_len:\n",
    "            padded.append(line + [1] * (max_len - len(line)))\n",
    "        else:\n",
    "            padded.append(line[: max_len])\n",
    "    padded = np.array(padded)\n",
    "\n",
    "    # print('padded:', padded[0])\n",
    "    # print('len padded:', padded.shape)\n",
    "\n",
    "    # ÄÃ¡nh dáº¥u cÃ¡c tá»« thÃªm vÃ o = 0 Ä‘á»ƒ khÃ´ng tÃ­nh vÃ o quÃ¡ trÃ¬nh láº¥y features\n",
    "    attention_mask = np.where(padded == 1, 0, 1)\n",
    "    # print('attention mask:', attention_mask[0])\n",
    "\n",
    "    # Chuyá»ƒn thÃ nh tensor\n",
    "    padded = torch.tensor(padded).to(torch.long)\n",
    "    print(\"Padd = \",padded.size())\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    # Láº¥y features dáº§u ra tá»« BERT\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model_trans(input_ids= padded, attention_mask=attention_mask)\n",
    "\n",
    "    v_features = last_hidden_states[0][:, 0, :].numpy()\n",
    "    print(v_features.shape)\n",
    "    return v_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padd =  torch.Size([1948, 150])\n",
      "(1948, 768)\n"
     ]
    }
   ],
   "source": [
    "X_train = make_bert_features(X_train)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh SVM\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padd =  torch.Size([487, 150])\n",
      "(487, 768)\n"
     ]
    }
   ],
   "source": [
    "# Chuáº©n bá»‹ dá»¯ liá»‡u kiá»ƒm tra\n",
    "X_test = make_bert_features(X_test)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8752260397830017\n"
     ]
    }
   ],
   "source": [
    "# Dá»± Ä‘oÃ¡n nhÃ£n cho táº­p kiá»ƒm tra\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best prarams: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "F1 score: 0.92573402417962\n"
     ]
    }
   ],
   "source": [
    "# Tune model báº±ng grid search\n",
    "parameters = {\n",
    "                'kernel': ('linear', 'rbf'), \n",
    "                'C': [0.5, 1, 2, 4], \n",
    "                'gamma': ['scale']#, 0.125, 0.25, 0.5, 1, 2, 4]\n",
    "            }\n",
    "\n",
    "clf = GridSearchCV(SVC(), param_grid=parameters,  cv = 5, scoring='f1', n_jobs=-1)\n",
    "grid_search = clf.fit(X_train, y_train)\n",
    "\n",
    "# best prarams\n",
    "print('best prarams:', clf.best_params_)\n",
    "\n",
    "svm_best_model = grid_search.best_estimator_\n",
    "y_pred = svm_best_model.predict(X_test)\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# dump(svm_model, 'save_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9116607773851589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Dá»± Ä‘oÃ¡n nhÃ£n cho táº­p kiá»ƒm tra\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Score: 0.9165439438601615\n",
      "F1 score: 0.9263157894736843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "15 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.90024343 0.89766168 0.91654394 0.91645879 0.9160616\n",
      "        nan 0.90077923 0.90613499 0.90053059 0.90022751 0.9033783\n",
      "        nan 0.8815877  0.89530175 0.88924228 0.88935487 0.89406021]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# Thiáº¿t láº­p lÆ°á»›i tham sá»‘ cáº§n tÃ¬m kiáº¿m\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# TÃ¬m kiáº¿m lÆ°á»›i cÃ¡c tham sá»‘ tá»‘t nháº¥t\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# In ra bá»™ tham sá»‘ tá»‘t nháº¥t vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u kiá»ƒm tra\n",
    "y_pred = grid_search.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "Best score:  0.8910752157897355\n",
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Äá»‹nh nghÄ©a cÃ¡c tham sá»‘ cáº§n tinh chá»‰nh vÃ  giÃ¡ trá»‹ Ä‘á»ƒ thá»­ nghiá»‡m\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "# Táº¡o Ä‘á»‘i tÆ°á»£ng Grid Search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv = 5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "# Tiáº¿n hÃ nh Grid Search trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# In ra cÃ¡c thÃ´ng sá»‘ tá»‘t nháº¥t tá»« Grid Search\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "# Dá»± Ä‘oÃ¡n nhÃ£n cho dá»¯ liá»‡u kiá»ƒm tra sá»­ dá»¥ng mÃ´ hÃ¬nh tá»‘t nháº¥t\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c\n",
    "accuracy = f1_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9204152249134949\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# Khá»Ÿi táº¡o mÃ´ hÃ¬nh XGBClassifier\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn táº­p huáº¥n luyá»‡n\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Dá»± Ä‘oÃ¡n nhÃ£n cho táº­p kiá»ƒm tra\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh\n",
    "accuracy = f1_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBClassifier()\n",
    "\n",
    "# # Äá»‹nh nghÄ©a siÃªu tham sá»‘ vÃ  giÃ¡ trá»‹ Ä‘á»ƒ tinh chá»‰nh\n",
    "# param_grid = {\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'learning_rate': [0.1, 0.01, 0.001],\n",
    "#     'n_estimators': [100, 500, 1000]\n",
    "# }\n",
    "\n",
    "# # Sá»­ dá»¥ng Grid Search Ä‘á»ƒ tÃ¬m siÃªu tham sá»‘ tá»‘t nháº¥t\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='f1', cv=3, n_jobs=4)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Láº¥y siÃªu tham sá»‘ tá»‘t nháº¥t vÃ  táº¡o mÃ´ hÃ¬nh vá»›i siÃªu tham sá»‘ Ä‘Ã³\n",
    "# best_params = grid_search.best_params_\n",
    "# model = XGBClassifier(**best_params)\n",
    "\n",
    "# # Huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn toÃ n bá»™ táº­p huáº¥n luyá»‡n\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Dá»± Ä‘oÃ¡n nhÃ£n cho táº­p kiá»ƒm tra\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh\n",
    "# accuracy = f1_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
