{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wctNkvsTMzBW"
      },
      "source": [
        "# **Äá»“ Ã¡n cuá»‘i kÃ¬**\n",
        "### **MÃ£ lá»›p:** CS114.N21\n",
        "\n",
        "### **Giáº£ng viÃªn:** Pháº¡m Nguyá»…n TrÆ°á»ng An\n",
        "### **Äá» tÃ i:** PhÃ¢n loáº¡i Ä‘Ã¡nh giÃ¡ cá»§a khÃ¡ch hÃ ng trÃªn Shopee\n",
        "\n",
        "### **ThÃ nh viÃªn nhÃ³m H2O:**\n",
        "- Nguyá»…n Quá»‘c Huy HoÃ ng - 20520051\n",
        "- NgÃ´ Quang Vinh - 19522523"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdD2F4FQNRMr"
      },
      "source": [
        "Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8EqEU6VcpyXx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "import re\n",
        "from pprint import pprint\n",
        "import torch\n",
        "import underthesea # ThÆ° viá»‡n tÃ¡ch tá»«\n",
        "from pyvi import ViTokenizer, ViPosTagger # thÆ° viá»‡n NLP tiáº¿ng Viá»‡t\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from joblib import dump"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-i_QqeMpyX0"
      },
      "source": [
        "## Xá»­ lÃ½ dá»¯ liá»‡u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s5RM8cXOEQU"
      },
      "source": [
        "### Äá»c dá»¯ liá»‡u sau Ä‘Ã³ chia lÃ m 2 táº­p train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Wv51jxrCpyX1",
        "outputId": "c28fb44d-b0d2-4428-8285-8f7e549aa8f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'X_train[0]'\n",
            "'KÃªu 1 cáº·p 2 cÃ¡i Ä‘áº·t x5 mÃ  giao Ä‘c 5 cÃ¡i shop lÃ m Äƒn kiá»ƒu gÃ¬ v'\n"
          ]
        }
      ],
      "source": [
        "# Äá»c dá»¯ liá»‡u tá»« file Excel\n",
        "file_path = \"dataset.xlsx\"\n",
        "data_frame = pd.read_excel(file_path, header=None)\n",
        "\n",
        "# Truy cáº­p vÃ o cá»™t thá»© nháº¥t vÃ  thá»© hai\n",
        "labels = data_frame.iloc[:, 0].tolist()\n",
        "reviews = data_frame.iloc[:, 1].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=20520051, shuffle=True)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "pprint(\"X_train[0]\")\n",
        "pprint(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fGzMWnikpyX2",
        "outputId": "24d8e86e-be83-4bdb-e069-3580dae9493c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ™‚ğŸ‘¿ğŸ˜ ğŸ˜¡ğŸ™ğŸ˜’ğŸ˜«ğŸ˜•ğŸ˜©ğŸ˜£ğŸ˜ŸğŸ˜­ğŸ˜¢ğŸ˜–ğŸ˜”ğŸ˜â˜¹ï¸ğŸ¤§ğŸ¤’ğŸ˜·ğŸ¤•ğŸ˜µğŸ¤¢ğŸ¤ ğŸ¤¡ğŸ‘¹ğŸ‘ºğŸ‘»ğŸ’€ğŸ‘½ğŸ˜°ğŸ˜¨ğŸ˜§ğŸ¤¥ğŸ˜ˆğŸ™ƒğŸ‘ğŸ¤¬ğŸ‘‹ğŸ˜ŒğŸ˜ğŸ˜¬ğŸ’©ğŸ˜“ğŸ˜¤ğŸ˜®â€ğŸ’¨ğŸ˜‘\n",
            "ğŸ˜€ğŸ˜ŠğŸ˜‰ğŸ˜ğŸ˜˜ğŸ˜—ğŸ˜™ğŸ¤—ğŸ˜šğŸ˜›ğŸ˜ğŸ˜œğŸ˜‹ğŸ¤‘ğŸ˜ğŸ˜‡â™¥ï¸â¤ï¸ğŸ’›ğŸ’šğŸ’™ğŸ’œğŸ–¤ğŸ’–ğŸ’ğŸ¤©ğŸ¥°ğŸ˜…ğŸ’ğŸ’‹ğŸ¤—ğŸ˜‹ğŸ¥³\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tichcuc = \"sáº£n pháº©m tá»‘t cháº¥t lÆ°á»£ng tá»‘t \"\n",
        "tieucuc = \"sáº£n pháº©m tá»‡ kÃ©m cháº¥t lÆ°á»£ng \"\n",
        "with open('negative icon.txt', 'r', encoding='utf-8') as file:\n",
        "    icon_tieu_cuc = file.read()\n",
        "print(icon_tieu_cuc)\n",
        "with open('positive_icon.txt', 'r', encoding='utf-8') as file:\n",
        "    icon_tich_cuc = file.read()\n",
        "print(icon_tich_cuc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjV3568wNdeS"
      },
      "source": [
        "### Chuáº©n hÃ³a dá»¯ liá»‡u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rkcYPYP-pyX3"
      },
      "outputs": [],
      "source": [
        "# HÃ m chuáº©n hoÃ¡ cÃ¢u\n",
        "def standardize_data(row):\n",
        "    # Icon biá»ƒu tÆ°á»£ng\n",
        "    # for val in row:\n",
        "    #     if val in icon_tich_cuc:\n",
        "    #         row = tichcuc + row\n",
        "    #     if val in icon_tieu_cuc:\n",
        "    #         row = tieucuc + row\n",
        "    # XoÃ¡ háº¿t nhá»¯ng cÃ¡i khÃ´ng pháº£i chá»¯ vÃ  sá»‘\n",
        "    row =  re.sub(r\"[^\\w\\s]\", \" \", row)\n",
        "\n",
        "    # # XÃ³a dáº¥u cháº¥m, pháº©y, há»i á»Ÿ cuá»‘i cÃ¢u\n",
        "    # row = re.sub(r\"[\\.,\\?]+$-\", \"\", row)\n",
        "    # # XÃ³a táº¥t cáº£ dáº¥u cháº¥m, pháº©y, cháº¥m pháº©y, cháº¥m thang, ... trong cÃ¢u\n",
        "    # row = row.replace(\",\", \" \").replace(\".\", \" \") \\\n",
        "    #     .replace(\";\", \" \").replace(\"â€œ\", \" \") \\\n",
        "    #     .replace(\":\", \" \").replace(\"â€\", \" \") \\\n",
        "    #     .replace('\"', \" \").replace(\"'\", \" \") \\\n",
        "    #     .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
        "    #     .replace(\"-\", \" \").replace(\"?\", \" \")\n",
        "    # # Náº¿u cÃ³ nhiá»u khoáº£ng tráº¯ng thÃ¬ rÃºt gá»n cÃ²n 1\n",
        "    row = row.replace('\\n',\" \")\n",
        "    row = re.sub(r\"\\s+\", \" \", row)\n",
        "    # ÄÆ°a háº¿t vá» chá»¯ thÆ°á»ng\n",
        "    row = row.strip().lower()\n",
        "    str = \"\"\n",
        "    str += row[0]\n",
        "    for i in range(1, len(row)):\n",
        "        if row[i] != row[i - 1]:\n",
        "            str += row[i]\n",
        "    # pprint(str)\n",
        "    # print(ascii(row[16]))\n",
        "    # Chuyá»ƒn thÃ nh dáº¡ng khÃ´ng dáº¥u\n",
        "    # str = unidecode(str)\n",
        "    return str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwTEov1bNhS2"
      },
      "source": [
        "### Láº¥y danh sÃ¡ch stop word cá»§a tiáº¿ng Viá»‡t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WAYdoQcwpyX3"
      },
      "outputs": [],
      "source": [
        "# HÃ m load danh sÃ¡ch cÃ¡c tá»« vÃ´ nghÄ©a: láº¯m, áº¡, Ã , bá»‹, vÃ¬..\n",
        "sw = []\n",
        "with open(\"vietnamese-stopwords.txt\", encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "for line in lines:\n",
        "    sw.append(line.replace(\"\\n\",\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sbyP5agNnuo"
      },
      "source": [
        "### Chuyá»ƒn vÄƒn báº£ng thÃ nh dáº¡ng vetor Ä‘áº·c trÆ°ng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "A4oQTnempyX4",
        "outputId": "02541962-5a04-4c8d-f0f1-c865a01a99e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at Fsoft-AIC/videberta-xsmall were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'deberta.embeddings.word_embeddings._weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at Fsoft-AIC/videberta-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'deberta.embeddings.word_embeddings._weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at vinai/phobert-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "list_model_trans = []\n",
        "list_tokenizer = []\n",
        "list_name_trans = []\n",
        "\n",
        "list_model_trans.append(AutoModel.from_pretrained(\"Fsoft-AIC/videberta-xsmall\"))\n",
        "list_tokenizer.append(AutoTokenizer.from_pretrained(\"Fsoft-AIC/videberta-xsmall\"))\n",
        "list_name_trans.append('videberta-xsmall')\n",
        "\n",
        "list_model_trans.append(AutoModel.from_pretrained(\"Fsoft-AIC/videberta-base\"))\n",
        "list_tokenizer.append(AutoTokenizer.from_pretrained(\"Fsoft-AIC/videberta-base\"))\n",
        "list_name_trans.append('videberta-base')\n",
        "\n",
        "list_model_trans.append(AutoModel.from_pretrained(\"vinai/phobert-base\"))\n",
        "list_tokenizer.append(AutoTokenizer.from_pretrained(\"vinai/phobert-base\"))\n",
        "list_name_trans.append('phobert-base')\n",
        "\n",
        "list_model_trans.append(AutoModel.from_pretrained(\"vinai/phobert-large\"))\n",
        "list_tokenizer.append(AutoTokenizer.from_pretrained(\"vinai/phobert-large\"))\n",
        "list_name_trans.append('phobert-large')\n",
        "\n",
        "list_model_trans.append(AutoModel.from_pretrained(\"vinai/phobert-base-v2\"))\n",
        "list_tokenizer.append(AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\"))\n",
        "list_name_trans.append('phobert-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aio-du7xpyX4"
      },
      "outputs": [],
      "source": [
        "# HÃ m táº¡o ra bert features\n",
        "def make_bert_features(v_text, model_trans, tokenizer):\n",
        "    v_tokenized = []\n",
        "    max_len = 150 # Má»—i cÃ¢u dÃ i tá»‘i Ä‘a 200 tá»«\n",
        "    for i_text in v_text:\n",
        "        # print(\"Äang xá»­ lÃ½ line = \", i_text)\n",
        "        # Chuáº©n hÃ³a\n",
        "\n",
        "        i_text = i_text.replace(\"\\n\",\" \")\n",
        "        i_text = standardize_data(i_text)\n",
        "\n",
        "        # # PhÃ¢n thÃ nh tá»«ng tá»«\n",
        "        # line = underthesea.word_tokenize(i_text)\n",
        "\n",
        "        # # Lá»c cÃ¡c tá»« vÃ´ nghÄ©a\n",
        "        # # filtered_sentence = [w for w in line if not w in sw]\n",
        "        # filtered_sentence = line\n",
        "\n",
        "        # # GhÃ©p láº¡i thÃ nh cÃ¢u nhÆ° cÅ© sau khi lá»c\n",
        "        # line = \" \".join(filtered_sentence[:150])\n",
        "        # line = underthesea.word_tokenize(line, format=\"text\")\n",
        "\n",
        "        line = ViTokenizer.tokenize(i_text)[:150]\n",
        "\n",
        "\n",
        "        # print(\"Word segment  = \", line)\n",
        "        # Tokenize bá»Ÿi BERT\n",
        "        line = tokenizer.encode(line)\n",
        "        v_tokenized.append(line)\n",
        "\n",
        "    padded = []\n",
        "    # ChÃ¨n thÃªm sá»‘ 1 vÃ o cuá»‘i cÃ¢u náº¿u nhÆ° khÃ´ng Ä‘á»§ tá»« hoáº·c xÃ³a náº¿u dÆ°\n",
        "    for line in v_tokenized:\n",
        "        if len(line) < max_len:\n",
        "            padded.append(line + [1] * (max_len - len(line)))\n",
        "        else:\n",
        "            padded.append(line[: max_len])\n",
        "    padded = np.array(padded)\n",
        "\n",
        "    # print('padded:', padded[0])\n",
        "    # print('len padded:', padded.shape)\n",
        "\n",
        "    # ÄÃ¡nh dáº¥u cÃ¡c tá»« thÃªm vÃ o = 0 Ä‘á»ƒ khÃ´ng tÃ­nh vÃ o quÃ¡ trÃ¬nh láº¥y features\n",
        "    attention_mask = np.where(padded == 1, 0, 1)\n",
        "    # print('attention mask:', attention_mask[0])\n",
        "\n",
        "    # Chuyá»ƒn thÃ nh tensor\n",
        "    padded = torch.tensor(padded).to(torch.long)\n",
        "    print(\"Padd = \",padded.size())\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "    # Láº¥y features dáº§u ra tá»« BERT\n",
        "    with torch.no_grad():\n",
        "        last_hidden_states = model_trans(input_ids= padded, attention_mask=attention_mask)\n",
        "\n",
        "    v_features = last_hidden_states[0][:, 0, :].numpy()\n",
        "    print(v_features.shape)\n",
        "    v_features = np.array(v_features)\n",
        "    return v_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Re_Va9PNs9N"
      },
      "source": [
        "## MÃ´ hÃ¬nh phÃ¢n lá»›p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve(model, parameters, X_train, y_train, X_test, y_test):\n",
        "    # Grid search Ä‘á»ƒ tÃ¬m bá»™ siÃªu tham sá»‘ tá»‘t nháº¥t\n",
        "    clf = GridSearchCV(model, param_grid=parameters,  cv = 5, scoring='f1', n_jobs=-1)\n",
        "    grid_search = clf.fit(X_train, y_train)\n",
        "\n",
        "    # In ra bá»™ siÃªu tham sá»‘ tÃ¬m Ä‘Æ°á»£c\n",
        "    print('best prarams:', clf.best_params_)\n",
        "\n",
        "    # DÃ¹ng model tá»‘t nháº¥t tÃ¬m Ä‘Æ°á»£c Ä‘á»ƒ dá»± Ä‘oÃ¡n\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(\"Model: \"+ type(model).__name__+\",\", \"F1 score:\", f1)\n",
        "    \n",
        "    # Save model\n",
        "    dump(best_model, type(model).__name__+'.joblib')\n",
        "\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im4ERO91N51l"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config.append(\n",
        "    (SVC(),{\n",
        "        'kernel': ('linear', 'rbf'),\n",
        "        'C': [0.5, 1, 2, 4],\n",
        "        'gamma': ['scale']#, 0.125, 0.25, 0.5, 1, 2, 4]\n",
        "    }))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RuqPKnqOVjI"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config.append(\n",
        "    (LogisticRegression(),{\n",
        "        'C': [0.1, 1, 10],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['lbfgs', 'liblinear', 'saga']\n",
        "    }))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd-qqLEnO6wD"
      },
      "source": [
        "### Radom Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config.append(\n",
        "    (RandomForestClassifier(),{\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cháº¡y thá»±c nghiá»‡m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padd =  torch.Size([1948, 150])\n",
            "(1948, 384)\n",
            "Padd =  torch.Size([487, 150])\n",
            "(487, 384)\n",
            "best prarams: {'C': 0.5, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "Model: SVC, F1 score: 0.7334200260078023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501\n",
            "        nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501\n",
            "        nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Model: LogisticRegression, F1 score: 0.7334200260078023\n",
            "best prarams: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Model: RandomForestClassifier, F1 score: 0.7334200260078023\n",
            "Padd =  torch.Size([1948, 150])\n",
            "(1948, 768)\n",
            "Padd =  torch.Size([487, 150])\n",
            "(487, 768)\n",
            "best prarams: {'C': 0.5, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "Model: SVC, F1 score: 0.7334200260078023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501\n",
            "        nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501\n",
            "        nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Model: LogisticRegression, F1 score: 0.7334200260078023\n",
            "best prarams: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Model: RandomForestClassifier, F1 score: 0.7334200260078023\n",
            "Padd =  torch.Size([1948, 150])\n",
            "(1948, 768)\n",
            "Padd =  torch.Size([487, 150])\n",
            "(487, 768)\n",
            "best prarams: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Model: SVC, F1 score: 0.9324090121317158\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.90555012 0.90238011 0.92165369 0.91936469 0.92033307\n",
            "        nan 0.90482247 0.90738237 0.90381735 0.90422592 0.90423842\n",
            "        nan 0.88255417 0.8999883  0.8921207  0.89113926 0.89979918]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Model: LogisticRegression, F1 score: 0.930795847750865\n",
            "best prarams: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Model: RandomForestClassifier, F1 score: 0.9134125636672327\n",
            "Padd =  torch.Size([1948, 150])\n",
            "(1948, 1024)\n",
            "Padd =  torch.Size([487, 150])\n",
            "(487, 1024)\n",
            "best prarams: {'C': 4, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "Model: SVC, F1 score: 0.9171075837742504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.72136501 0.72136501 0.7923272  0.79397976 0.7923272\n",
            "        nan 0.85646782 0.8606509  0.88180998 0.88180998 0.88180998\n",
            "        nan 0.90906917 0.91489715 0.91188025 0.91228989 0.91195683]\n",
            "  warnings.warn(\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 10, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Model: LogisticRegression, F1 score: 0.9148936170212766\n",
            "best prarams: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "Model: RandomForestClassifier, F1 score: 0.8797250859106529\n",
            "Padd =  torch.Size([1948, 150])\n",
            "(1948, 768)\n",
            "Padd =  torch.Size([487, 150])\n",
            "(487, 768)\n",
            "best prarams: {'C': 2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Model: SVC, F1 score: 0.9368421052631578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.9213887  0.92329808 0.94091666 0.94133239 0.94091666\n",
            "        nan 0.93414086 0.93526413 0.93345708 0.93491965 0.93491965\n",
            "        nan 0.91061667 0.92454932 0.92166555 0.92264091 0.92589328]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Model: LogisticRegression, F1 score: 0.9368421052631578\n",
            "best prarams: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Model: RandomForestClassifier, F1 score: 0.910321489001692\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>videberta-xsmall</th>\n",
              "      <th>videberta-base</th>\n",
              "      <th>phobert-base</th>\n",
              "      <th>phobert-large</th>\n",
              "      <th>phobert-base-v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.932409</td>\n",
              "      <td>0.917108</td>\n",
              "      <td>0.936842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.930796</td>\n",
              "      <td>0.914894</td>\n",
              "      <td>0.936842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.913413</td>\n",
              "      <td>0.879725</td>\n",
              "      <td>0.910321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   videberta-xsmall  videberta-base  phobert-base  phobert-large  \\\n",
              "0           0.73342         0.73342      0.932409       0.917108   \n",
              "1           0.73342         0.73342      0.930796       0.914894   \n",
              "2           0.73342         0.73342      0.913413       0.879725   \n",
              "\n",
              "   phobert-base-v2  \n",
              "0         0.936842  \n",
              "1         0.936842  \n",
              "2         0.910321  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = {}\n",
        "for i in range(len(list_model_trans)):\n",
        "    model_trans = list_model_trans[i]\n",
        "    tokenizer = list_tokenizer[i]\n",
        "    name_trans = list_name_trans[i]\n",
        "\n",
        "    # Chuyá»ƒn vÄƒn báº£n thÃ nh vector Ä‘áº·c trÆ°ng\n",
        "    new_X_train = make_bert_features(X_train, model_trans, tokenizer)\n",
        "    new_X_test = make_bert_features(X_test, model_trans, tokenizer)\n",
        "\n",
        "    value_f1 = []\n",
        "    for model, params in model_config:\n",
        "        f1 = solve(model, params, new_X_train, y_train, new_X_test, y_test)\n",
        "        value_f1.append(f1)\n",
        "        # break\n",
        "    df[name_trans] = value_f1\n",
        "    # break\n",
        "df = pd.DataFrame(df)\n",
        "df\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
