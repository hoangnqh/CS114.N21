{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wctNkvsTMzBW"
      },
      "source": [
        "# **Đồ án cuối kì**\n",
        "### **Mã lớp:** CS114.N21\n",
        "\n",
        "### **Giảng viên:** Phạm Nguyễn Trường An\n",
        "### **Đề tài:** Phân loại đánh giá của khách hàng trên Shopee\n",
        "\n",
        "### **Thành viên nhóm H2O:**\n",
        "- Nguyễn Quốc Huy Hoàng - 20520051\n",
        "- Ngô Quang Vinh - 19522523"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdD2F4FQNRMr"
      },
      "source": [
        "Import các thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8EqEU6VcpyXx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "import re\n",
        "from pprint import pprint\n",
        "import torch\n",
        "import underthesea # Thư viện tách từ\n",
        "from pyvi import ViTokenizer, ViPosTagger # thư viện NLP tiếng Việt\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from joblib import dump"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-i_QqeMpyX0"
      },
      "source": [
        "## Xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s5RM8cXOEQU"
      },
      "source": [
        "### Đọc dữ liệu sau đó chia làm 2 tập train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Wv51jxrCpyX1",
        "outputId": "c28fb44d-b0d2-4428-8285-8f7e549aa8f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'X_train[0]'\n",
            "'Kêu 1 cặp 2 cái đặt x5 mà giao đc 5 cái shop làm ăn kiểu gì v'\n"
          ]
        }
      ],
      "source": [
        "# Đọc dữ liệu từ file Excel\n",
        "file_path = \"dataset.xlsx\"\n",
        "data_frame = pd.read_excel(file_path, header=None)\n",
        "\n",
        "# Truy cập vào cột thứ nhất và thứ hai\n",
        "labels = data_frame.iloc[:, 0].tolist()\n",
        "reviews = data_frame.iloc[:, 1].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=20520051, shuffle=True)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "pprint(\"X_train[0]\")\n",
        "pprint(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fGzMWnikpyX2",
        "outputId": "24d8e86e-be83-4bdb-e069-3580dae9493c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🙂👿😠😡🙁😒😫😕😩😣😟😭😢😖😔😞☹️🤧🤒😷🤕😵🤢🤠🤡👹👺👻💀👽😰😨😧🤥😈🙃👎🤬👋😌😏😬💩😓😤😮‍💨😑\n",
            "😀😊😉😍😘😗😙🤗😚😛😝😜😋🤑😎😇♥️❤️💛💚💙💜🖤💖💝🤩🥰😅💞💋🤗😋🥳\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tichcuc = \"sản phẩm tốt chất lượng tốt \"\n",
        "tieucuc = \"sản phẩm tệ kém chất lượng \"\n",
        "with open('negative icon.txt', 'r', encoding='utf-8') as file:\n",
        "    icon_tieu_cuc = file.read()\n",
        "print(icon_tieu_cuc)\n",
        "with open('positive_icon.txt', 'r', encoding='utf-8') as file:\n",
        "    icon_tich_cuc = file.read()\n",
        "print(icon_tich_cuc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjV3568wNdeS"
      },
      "source": [
        "### Chuẩn hóa dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rkcYPYP-pyX3"
      },
      "outputs": [],
      "source": [
        "# Hàm chuẩn hoá câu\n",
        "def standardize_data(row):\n",
        "    # Icon biểu tượng\n",
        "    # for val in row:\n",
        "    #     if val in icon_tich_cuc:\n",
        "    #         row = tichcuc + row\n",
        "    #     if val in icon_tieu_cuc:\n",
        "    #         row = tieucuc + row\n",
        "    # Xoá hết những cái không phải chữ và số\n",
        "    row =  re.sub(r\"[^\\w\\s]\", \" \", row)\n",
        "\n",
        "    # # Xóa dấu chấm, phẩy, hỏi ở cuối câu\n",
        "    # row = re.sub(r\"[\\.,\\?]+$-\", \"\", row)\n",
        "    # # Xóa tất cả dấu chấm, phẩy, chấm phẩy, chấm thang, ... trong câu\n",
        "    # row = row.replace(\",\", \" \").replace(\".\", \" \") \\\n",
        "    #     .replace(\";\", \" \").replace(\"“\", \" \") \\\n",
        "    #     .replace(\":\", \" \").replace(\"”\", \" \") \\\n",
        "    #     .replace('\"', \" \").replace(\"'\", \" \") \\\n",
        "    #     .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
        "    #     .replace(\"-\", \" \").replace(\"?\", \" \")\n",
        "    # # Nếu có nhiều khoảng trắng thì rút gọn còn 1\n",
        "    row = row.replace('\\n',\" \")\n",
        "    row = re.sub(r\"\\s+\", \" \", row)\n",
        "    # Đưa hết về chữ thường\n",
        "    row = row.strip().lower()\n",
        "    str = \"\"\n",
        "    str += row[0]\n",
        "    for i in range(1, len(row)):\n",
        "        if row[i] != row[i - 1]:\n",
        "            str += row[i]\n",
        "    # pprint(str)\n",
        "    # print(ascii(row[16]))\n",
        "    # Chuyển thành dạng không dấu\n",
        "    # str = unidecode(str)\n",
        "    return str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwTEov1bNhS2"
      },
      "source": [
        "### Lấy danh sách stop word của tiếng Việt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WAYdoQcwpyX3"
      },
      "outputs": [],
      "source": [
        "# Hàm load danh sách các từ vô nghĩa: lắm, ạ, à, bị, vì..\n",
        "sw = []\n",
        "with open(\"vietnamese-stopwords.txt\", encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "for line in lines:\n",
        "    sw.append(line.replace(\"\\n\",\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sbyP5agNnuo"
      },
      "source": [
        "### Chuyển văn bảng thành dạng vetor đặc trưng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "A4oQTnempyX4",
        "outputId": "02541962-5a04-4c8d-f0f1-c865a01a99e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at Fsoft-AIC/videberta-xsmall were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'deberta.embeddings.word_embeddings._weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at Fsoft-AIC/videberta-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'deberta.embeddings.word_embeddings._weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at vinai/phobert-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "list_model_trans = []\n",
        "list_tokenizer = []\n",
        "list_name_trans = []\n",
        "\n",
        "list_model_trans.append(AutoModel.from_pretrained(\"Fsoft-AIC/videberta-xsmall\"))\n",
        "list_tokenizer.append(AutoTokenizer.from_pretrained(\"Fsoft-AIC/videberta-xsmall\"))\n",
        "list_name_trans.append('videberta-xsmall')\n",
        "\n",
        "list_model_trans.append(AutoModel.from_pretrained(\"Fsoft-AIC/videberta-base\"))\n",
        "list_tokenizer.append(AutoTokenizer.from_pretrained(\"Fsoft-AIC/videberta-base\"))\n",
        "list_name_trans.append('videberta-base')\n",
        "\n",
        "list_model_trans.append(AutoModel.from_pretrained(\"vinai/phobert-base\"))\n",
        "list_tokenizer.append(AutoTokenizer.from_pretrained(\"vinai/phobert-base\"))\n",
        "list_name_trans.append('phobert-base')\n",
        "\n",
        "list_model_trans.append(AutoModel.from_pretrained(\"vinai/phobert-large\"))\n",
        "list_tokenizer.append(AutoTokenizer.from_pretrained(\"vinai/phobert-large\"))\n",
        "list_name_trans.append('phobert-large')\n",
        "\n",
        "list_model_trans.append(AutoModel.from_pretrained(\"vinai/phobert-base-v2\"))\n",
        "list_tokenizer.append(AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\"))\n",
        "list_name_trans.append('phobert-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aio-du7xpyX4"
      },
      "outputs": [],
      "source": [
        "# Hàm tạo ra bert features\n",
        "def make_bert_features(v_text, model_trans, tokenizer):\n",
        "    v_tokenized = []\n",
        "    max_len = 150 # Mỗi câu dài tối đa 200 từ\n",
        "    for i_text in v_text:\n",
        "        # print(\"Đang xử lý line = \", i_text)\n",
        "        # Chuẩn hóa\n",
        "\n",
        "        i_text = i_text.replace(\"\\n\",\" \")\n",
        "        i_text = standardize_data(i_text)\n",
        "\n",
        "        # # Phân thành từng từ\n",
        "        # line = underthesea.word_tokenize(i_text)\n",
        "\n",
        "        # # Lọc các từ vô nghĩa\n",
        "        # # filtered_sentence = [w for w in line if not w in sw]\n",
        "        # filtered_sentence = line\n",
        "\n",
        "        # # Ghép lại thành câu như cũ sau khi lọc\n",
        "        # line = \" \".join(filtered_sentence[:150])\n",
        "        # line = underthesea.word_tokenize(line, format=\"text\")\n",
        "\n",
        "        line = ViTokenizer.tokenize(i_text)[:150]\n",
        "\n",
        "\n",
        "        # print(\"Word segment  = \", line)\n",
        "        # Tokenize bởi BERT\n",
        "        line = tokenizer.encode(line)\n",
        "        v_tokenized.append(line)\n",
        "\n",
        "    padded = []\n",
        "    # Chèn thêm số 1 vào cuối câu nếu như không đủ từ hoặc xóa nếu dư\n",
        "    for line in v_tokenized:\n",
        "        if len(line) < max_len:\n",
        "            padded.append(line + [1] * (max_len - len(line)))\n",
        "        else:\n",
        "            padded.append(line[: max_len])\n",
        "    padded = np.array(padded)\n",
        "\n",
        "    # print('padded:', padded[0])\n",
        "    # print('len padded:', padded.shape)\n",
        "\n",
        "    # Đánh dấu các từ thêm vào = 0 để không tính vào quá trình lấy features\n",
        "    attention_mask = np.where(padded == 1, 0, 1)\n",
        "    # print('attention mask:', attention_mask[0])\n",
        "\n",
        "    # Chuyển thành tensor\n",
        "    padded = torch.tensor(padded).to(torch.long)\n",
        "    print(\"Padd = \",padded.size())\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "    # Lấy features dầu ra từ BERT\n",
        "    with torch.no_grad():\n",
        "        last_hidden_states = model_trans(input_ids= padded, attention_mask=attention_mask)\n",
        "\n",
        "    v_features = last_hidden_states[0][:, 0, :].numpy()\n",
        "    print(v_features.shape)\n",
        "    v_features = np.array(v_features)\n",
        "    return v_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Re_Va9PNs9N"
      },
      "source": [
        "## Mô hình phân lớp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve(model, parameters, X_train, y_train, X_test, y_test):\n",
        "    # Grid search để tìm bộ siêu tham số tốt nhất\n",
        "    clf = GridSearchCV(model, param_grid=parameters,  cv = 5, scoring='f1', n_jobs=-1)\n",
        "    grid_search = clf.fit(X_train, y_train)\n",
        "\n",
        "    # In ra bộ siêu tham số tìm được\n",
        "    print('best prarams:', clf.best_params_)\n",
        "\n",
        "    # Dùng model tốt nhất tìm được để dự đoán\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Đánh giá mô hình\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(\"Model: \"+ type(model).__name__+\",\", \"F1 score:\", f1)\n",
        "    \n",
        "    # Save model\n",
        "    dump(best_model, type(model).__name__+'.joblib')\n",
        "\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im4ERO91N51l"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config.append(\n",
        "    (SVC(),{\n",
        "        'kernel': ('linear', 'rbf'),\n",
        "        'C': [0.5, 1, 2, 4],\n",
        "        'gamma': ['scale']#, 0.125, 0.25, 0.5, 1, 2, 4]\n",
        "    }))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RuqPKnqOVjI"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config.append(\n",
        "    (LogisticRegression(),{\n",
        "        'C': [0.1, 1, 10],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['lbfgs', 'liblinear', 'saga']\n",
        "    }))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd-qqLEnO6wD"
      },
      "source": [
        "### Radom Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_config.append(\n",
        "    (RandomForestClassifier(),{\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 5, 10],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chạy thực nghiệm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padd =  torch.Size([1948, 150])\n",
            "(1948, 384)\n",
            "Padd =  torch.Size([487, 150])\n",
            "(487, 384)\n",
            "best prarams: {'C': 0.5, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "Model: SVC, F1 score: 0.7334200260078023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501\n",
            "        nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501\n",
            "        nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Model: LogisticRegression, F1 score: 0.7334200260078023\n",
            "best prarams: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Model: RandomForestClassifier, F1 score: 0.7334200260078023\n",
            "Padd =  torch.Size([1948, 150])\n",
            "(1948, 768)\n",
            "Padd =  torch.Size([487, 150])\n",
            "(487, 768)\n",
            "best prarams: {'C': 0.5, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "Model: SVC, F1 score: 0.7334200260078023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501\n",
            "        nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501\n",
            "        nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Model: LogisticRegression, F1 score: 0.7334200260078023\n",
            "best prarams: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Model: RandomForestClassifier, F1 score: 0.7334200260078023\n",
            "Padd =  torch.Size([1948, 150])\n",
            "(1948, 768)\n",
            "Padd =  torch.Size([487, 150])\n",
            "(487, 768)\n",
            "best prarams: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Model: SVC, F1 score: 0.9324090121317158\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.90555012 0.90238011 0.92165369 0.91936469 0.92033307\n",
            "        nan 0.90482247 0.90738237 0.90381735 0.90422592 0.90423842\n",
            "        nan 0.88255417 0.8999883  0.8921207  0.89113926 0.89979918]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Model: LogisticRegression, F1 score: 0.930795847750865\n",
            "best prarams: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Model: RandomForestClassifier, F1 score: 0.9134125636672327\n",
            "Padd =  torch.Size([1948, 150])\n",
            "(1948, 1024)\n",
            "Padd =  torch.Size([487, 150])\n",
            "(487, 1024)\n",
            "best prarams: {'C': 4, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "Model: SVC, F1 score: 0.9171075837742504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.72136501 0.72136501 0.7923272  0.79397976 0.7923272\n",
            "        nan 0.85646782 0.8606509  0.88180998 0.88180998 0.88180998\n",
            "        nan 0.90906917 0.91489715 0.91188025 0.91228989 0.91195683]\n",
            "  warnings.warn(\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 10, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Model: LogisticRegression, F1 score: 0.9148936170212766\n",
            "best prarams: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "Model: RandomForestClassifier, F1 score: 0.8797250859106529\n",
            "Padd =  torch.Size([1948, 150])\n",
            "(1948, 768)\n",
            "Padd =  torch.Size([487, 150])\n",
            "(487, 768)\n",
            "best prarams: {'C': 2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Model: SVC, F1 score: 0.9368421052631578\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.9213887  0.92329808 0.94091666 0.94133239 0.94091666\n",
            "        nan 0.93414086 0.93526413 0.93345708 0.93491965 0.93491965\n",
            "        nan 0.91061667 0.92454932 0.92166555 0.92264091 0.92589328]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Model: LogisticRegression, F1 score: 0.9368421052631578\n",
            "best prarams: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Model: RandomForestClassifier, F1 score: 0.910321489001692\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>videberta-xsmall</th>\n",
              "      <th>videberta-base</th>\n",
              "      <th>phobert-base</th>\n",
              "      <th>phobert-large</th>\n",
              "      <th>phobert-base-v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.932409</td>\n",
              "      <td>0.917108</td>\n",
              "      <td>0.936842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.930796</td>\n",
              "      <td>0.914894</td>\n",
              "      <td>0.936842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.73342</td>\n",
              "      <td>0.913413</td>\n",
              "      <td>0.879725</td>\n",
              "      <td>0.910321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   videberta-xsmall  videberta-base  phobert-base  phobert-large  \\\n",
              "0           0.73342         0.73342      0.932409       0.917108   \n",
              "1           0.73342         0.73342      0.930796       0.914894   \n",
              "2           0.73342         0.73342      0.913413       0.879725   \n",
              "\n",
              "   phobert-base-v2  \n",
              "0         0.936842  \n",
              "1         0.936842  \n",
              "2         0.910321  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = {}\n",
        "for i in range(len(list_model_trans)):\n",
        "    model_trans = list_model_trans[i]\n",
        "    tokenizer = list_tokenizer[i]\n",
        "    name_trans = list_name_trans[i]\n",
        "\n",
        "    # Chuyển văn bản thành vector đặc trưng\n",
        "    new_X_train = make_bert_features(X_train, model_trans, tokenizer)\n",
        "    new_X_test = make_bert_features(X_test, model_trans, tokenizer)\n",
        "\n",
        "    value_f1 = []\n",
        "    for model, params in model_config:\n",
        "        f1 = solve(model, params, new_X_train, y_train, new_X_test, y_test)\n",
        "        value_f1.append(f1)\n",
        "        # break\n",
        "    df[name_trans] = value_f1\n",
        "    # break\n",
        "df = pd.DataFrame(df)\n",
        "df\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
