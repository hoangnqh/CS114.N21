{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Äá»“ Ã¡n cuá»‘i kÃ¬**\n",
        "### **MÃ£ lá»›p:** CS114.N21\n",
        "\n",
        "### **Giáº£ng viÃªn:** Pháº¡m Nguyá»…n TrÆ°á»ng An\n",
        "### **Äá» tÃ i:** PhÃ¢n loáº¡i Ä‘Ã¡nh giÃ¡ cá»§a khÃ¡ch hÃ ng trÃªn Shopee\n",
        "\n",
        "### **ThÃ nh viÃªn nhÃ³m H2O:**\n",
        "- Nguyá»…n Quá»‘c Huy HoÃ ng - 20520051\n",
        "- NgÃ´ Quang Vinh - 19522523"
      ],
      "metadata": {
        "id": "wctNkvsTMzBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t"
      ],
      "metadata": {
        "id": "RdD2F4FQNRMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EqEU6VcpyXx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers.tokenization_utils_base import TruncationStrategy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "import json\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import underthesea # ThÆ° viá»‡n tÃ¡ch tá»«\n",
        "from pyvi import ViTokenizer, ViPosTagger # thÆ° viá»‡n NLP tiáº¿ng Viá»‡t\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score\n",
        "from joblib import dump\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-i_QqeMpyX0"
      },
      "source": [
        "## Xá»­ lÃ½ dá»¯ liá»‡u"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Äá»c dá»¯ liá»‡u sau Ä‘Ã³ chia lÃ m 2 táº­p train/test"
      ],
      "metadata": {
        "id": "8s5RM8cXOEQU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wv51jxrCpyX1",
        "outputId": "c28fb44d-b0d2-4428-8285-8f7e549aa8f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'X_train[0]'\n",
            "'KÃªu 1 cáº·p 2 cÃ¡i Ä‘áº·t x5 mÃ  giao Ä‘c 5 cÃ¡i shop lÃ m Äƒn kiá»ƒu gÃ¬ v'\n"
          ]
        }
      ],
      "source": [
        "# Äá»c dá»¯ liá»‡u tá»« file Excel\n",
        "file_path = \"Book1.xlsx\"\n",
        "data_frame = pd.read_excel(file_path, header=None)\n",
        "\n",
        "# Truy cáº­p vÃ o cá»™t thá»© nháº¥t vÃ  thá»© hai\n",
        "labels = data_frame.iloc[:, 0].tolist()\n",
        "reviews = data_frame.iloc[:, 1].tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=20520051, shuffle=True)\n",
        "pprint(\"X_train[0]\")\n",
        "pprint(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGzMWnikpyX2",
        "outputId": "24d8e86e-be83-4bdb-e069-3580dae9493c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ™‚ğŸ‘¿ğŸ˜ ğŸ˜¡ğŸ™ğŸ˜’ğŸ˜«ğŸ˜•ğŸ˜©ğŸ˜£ğŸ˜ŸğŸ˜­ğŸ˜¢ğŸ˜–ğŸ˜”ğŸ˜â˜¹ï¸ğŸ¤§ğŸ¤’ğŸ˜·ğŸ¤•ğŸ˜µğŸ¤¢ğŸ¤ ğŸ¤¡ğŸ‘¹ğŸ‘ºğŸ‘»ğŸ’€ğŸ‘½ğŸ˜°ğŸ˜¨ğŸ˜§ğŸ¤¥ğŸ˜ˆğŸ™ƒğŸ‘ğŸ¤¬ğŸ‘‹ğŸ˜ŒğŸ˜ğŸ˜¬ğŸ’©ğŸ˜“ğŸ˜¤ğŸ˜®â€ğŸ’¨ğŸ˜‘\n",
            "ğŸ˜€ğŸ˜ŠğŸ˜‰ğŸ˜ğŸ˜˜ğŸ˜—ğŸ˜™ğŸ¤—ğŸ˜šğŸ˜›ğŸ˜ğŸ˜œğŸ˜‹ğŸ¤‘ğŸ˜ğŸ˜‡â™¥ï¸â¤ï¸ğŸ’›ğŸ’šğŸ’™ğŸ’œğŸ–¤ğŸ’–ğŸ’ğŸ¤©ğŸ¥°ğŸ˜…ğŸ’ğŸ’‹ğŸ¤—ğŸ˜‹ğŸ¥³\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tichcuc = \"sáº£n pháº©m tá»‘t cháº¥t lÆ°á»£ng tá»‘t \"\n",
        "tieucuc = \"sáº£n pháº©m tá»‡ kÃ©m cháº¥t lÆ°á»£ng \"\n",
        "with open('icon_tieu_cuc.txt', 'r', encoding='utf-8') as file:\n",
        "    icon_tieu_cuc = file.read()\n",
        "print(icon_tieu_cuc)\n",
        "with open('icon_tich_cuc.txt', 'r', encoding='utf-8') as file:\n",
        "    icon_tich_cuc = file.read()\n",
        "print(icon_tich_cuc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chuáº©n hÃ³a dá»¯ liá»‡u"
      ],
      "metadata": {
        "id": "FjV3568wNdeS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkcYPYP-pyX3"
      },
      "outputs": [],
      "source": [
        "# HÃ m chuáº©n hoÃ¡ cÃ¢u\n",
        "def standardize_data(row):\n",
        "    # Icon biá»ƒu tÆ°á»£ng\n",
        "    for val in row:\n",
        "        if val in icon_tich_cuc:\n",
        "            row = tichcuc + row\n",
        "        if val in icon_tieu_cuc:\n",
        "            row = tieucuc + row\n",
        "    # XoÃ¡ háº¿t nhá»¯ng cÃ¡i khÃ´ng pháº£i chá»¯ vÃ  sá»‘\n",
        "    row =  re.sub(r\"[^\\w\\s]\", \" \", row)\n",
        "\n",
        "    # # XÃ³a dáº¥u cháº¥m, pháº©y, há»i á»Ÿ cuá»‘i cÃ¢u\n",
        "    # row = re.sub(r\"[\\.,\\?]+$-\", \"\", row)\n",
        "    # # XÃ³a táº¥t cáº£ dáº¥u cháº¥m, pháº©y, cháº¥m pháº©y, cháº¥m thang, ... trong cÃ¢u\n",
        "    # row = row.replace(\",\", \" \").replace(\".\", \" \") \\\n",
        "    #     .replace(\";\", \" \").replace(\"â€œ\", \" \") \\\n",
        "    #     .replace(\":\", \" \").replace(\"â€\", \" \") \\\n",
        "    #     .replace('\"', \" \").replace(\"'\", \" \") \\\n",
        "    #     .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
        "    #     .replace(\"-\", \" \").replace(\"?\", \" \")\n",
        "    # # Náº¿u cÃ³ nhiá»u khoáº£ng tráº¯ng thÃ¬ rÃºt gá»n cÃ²n 1\n",
        "    row = row.replace('\\n',\" \")\n",
        "    row = re.sub(r\"\\s+\", \" \", row)\n",
        "    # ÄÆ°a háº¿t vá» chá»¯ thÆ°á»ng\n",
        "    row = row.strip().lower()\n",
        "    str = \"\"\n",
        "    str += row[0]\n",
        "    for i in range(1, len(row)):\n",
        "        if row[i] != row[i - 1]:\n",
        "            str += row[i]\n",
        "    # pprint(str)\n",
        "    # print(ascii(row[16]))\n",
        "    # Chuyá»ƒn thÃ nh dáº¡ng khÃ´ng dáº¥u\n",
        "    # str = unidecode(str)\n",
        "    return str"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Láº¥y danh sÃ¡ch stop word cá»§a tiáº¿ng Viá»‡t"
      ],
      "metadata": {
        "id": "gwTEov1bNhS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAYdoQcwpyX3"
      },
      "outputs": [],
      "source": [
        "# HÃ m load danh sÃ¡ch cÃ¡c tá»« vÃ´ nghÄ©a: láº¯m, áº¡, Ã , bá»‹, vÃ¬..\n",
        "sw = []\n",
        "with open(\"vietnamese-stopwords - Copy.txt\", encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "for line in lines:\n",
        "    sw.append(line.replace(\"\\n\",\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chuyá»ƒn vÄƒn báº£ng thÃ nh dáº¡ng vetor Ä‘áº·c trÆ°ng"
      ],
      "metadata": {
        "id": "4sbyP5agNnuo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4oQTnempyX4",
        "outputId": "02541962-5a04-4c8d-f0f1-c865a01a99e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# model_trans = AutoModel.from_pretrained(\"Fsoft-AIC/videberta-xsmall\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"Fsoft-AIC/videberta-xsmall\")\n",
        "\n",
        "# model_trans = AutoModel.from_pretrained(\"Fsoft-AIC/videberta-base\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"Fsoft-AIC/videberta-base\")\n",
        "\n",
        "model_trans = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "# model_trans = AutoModel.from_pretrained(\"vinai/phobert-large\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-large\")\n",
        "\n",
        "# model_trans = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aio-du7xpyX4"
      },
      "outputs": [],
      "source": [
        "# HÃ m táº¡o ra bert features\n",
        "def make_bert_features(v_text):\n",
        "    v_tokenized = []\n",
        "    max_len = 150 # Má»—i cÃ¢u dÃ i tá»‘i Ä‘a 200 tá»«\n",
        "    for i_text in v_text:\n",
        "        # print(\"Äang xá»­ lÃ½ line = \", i_text)\n",
        "        # Chuáº©n hÃ³a\n",
        "\n",
        "        i_text = i_text.replace(\"\\n\",\" \")\n",
        "        i_text = standardize_data(i_text)\n",
        "\n",
        "        # # PhÃ¢n thÃ nh tá»«ng tá»«\n",
        "        # line = underthesea.word_tokenize(i_text)\n",
        "\n",
        "        # # Lá»c cÃ¡c tá»« vÃ´ nghÄ©a\n",
        "        # # filtered_sentence = [w for w in line if not w in sw]\n",
        "        # filtered_sentence = line\n",
        "\n",
        "        # # GhÃ©p láº¡i thÃ nh cÃ¢u nhÆ° cÅ© sau khi lá»c\n",
        "        # line = \" \".join(filtered_sentence[:100])\n",
        "        # line = underthesea.word_tokenize(line, format=\"text\")\n",
        "        line = ViTokenizer.tokenize(i_text)[:150]\n",
        "\n",
        "\n",
        "        # print(\"Word segment  = \", line)\n",
        "        # Tokenize bá»Ÿi BERT\n",
        "        line = tokenizer.encode(line)\n",
        "        v_tokenized.append(line)\n",
        "\n",
        "    padded = []\n",
        "    # ChÃ¨n thÃªm sá»‘ 1 vÃ o cuá»‘i cÃ¢u náº¿u nhÆ° khÃ´ng Ä‘á»§ tá»« hoáº·c xÃ³a náº¿u dÆ°\n",
        "    for line in v_tokenized:\n",
        "        if len(line) < max_len:\n",
        "            padded.append(line + [1] * (max_len - len(line)))\n",
        "        else:\n",
        "            padded.append(line[: max_len])\n",
        "    padded = np.array(padded)\n",
        "\n",
        "    # print('padded:', padded[0])\n",
        "    # print('len padded:', padded.shape)\n",
        "\n",
        "    # ÄÃ¡nh dáº¥u cÃ¡c tá»« thÃªm vÃ o = 0 Ä‘á»ƒ khÃ´ng tÃ­nh vÃ o quÃ¡ trÃ¬nh láº¥y features\n",
        "    attention_mask = np.where(padded == 1, 0, 1)\n",
        "    # print('attention mask:', attention_mask[0])\n",
        "\n",
        "    # Chuyá»ƒn thÃ nh tensor\n",
        "    padded = torch.tensor(padded).to(torch.long)\n",
        "    print(\"Padd = \",padded.size())\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "    # Láº¥y features dáº§u ra tá»« BERT\n",
        "    with torch.no_grad():\n",
        "        last_hidden_states = model_trans(input_ids= padded, attention_mask=attention_mask)\n",
        "\n",
        "    v_features = last_hidden_states[0][:, 0, :].numpy()\n",
        "    print(v_features.shape)\n",
        "    return v_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5M0oBU-pyX4",
        "outputId": "1fdc47e2-e42b-4e4a-f3e1-724b85eb5d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padd =  torch.Size([1948, 150])\n"
          ]
        }
      ],
      "source": [
        "# Xá»­ lÃ½ dá»¯ liá»‡u táº­p train\n",
        "X_train = make_bert_features(X_train)\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqEDTFx3pyX5",
        "outputId": "3a778701-bfe0-4bb9-c15d-8a920ecc8613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padd =  torch.Size([487, 150])\n",
            "(487, 384)\n"
          ]
        }
      ],
      "source": [
        "# Xá»­ lÃ½ dá»¯ liá»‡u táº­p test\n",
        "X_test = make_bert_features(X_test)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MÃ´ hÃ¬nh phÃ¢n lá»›p\n"
      ],
      "metadata": {
        "id": "4Re_Va9PNs9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {
        "id": "Im4ERO91N51l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh"
      ],
      "metadata": {
        "id": "f1w_vdAVN7rs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkNkvU7_pyX5"
      },
      "outputs": [],
      "source": [
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dá»± Ä‘oÃ¡n vÃ  Ä‘Ã¡nh giÃ¡"
      ],
      "metadata": {
        "id": "tMwj5dnoOPuT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ4EyLffpyX5",
        "outputId": "4ae846ab-7153-446f-fe42-73601974b8be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.7334200260078023\n"
          ]
        }
      ],
      "source": [
        "# Dá»± Ä‘oÃ¡n nhÃ£n cho táº­p kiá»ƒm tra\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tinh chá»‰nh mÃ´ hÃ¬nh"
      ],
      "metadata": {
        "id": "7X8T1MGhOSl7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRgsSQ9QpyX5",
        "outputId": "024f9049-24b7-4fc9-dbb4-f5e9ee38fb77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best prarams: {'C': 0.5, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "F1 score: 0.7334200260078023\n"
          ]
        }
      ],
      "source": [
        "# Tune model báº±ng grid search\n",
        "parameters = {\n",
        "                'kernel': ('linear', 'rbf'),\n",
        "                'C': [0.5, 1, 2, 4],\n",
        "                'gamma': ['scale']#, 0.125, 0.25, 0.5, 1, 2, 4]\n",
        "            }\n",
        "\n",
        "clf = GridSearchCV(SVC(), param_grid=parameters,  cv = 5, scoring='f1', n_jobs=-1)\n",
        "grid_search = clf.fit(X_train, y_train)\n",
        "\n",
        "# best prarams\n",
        "print('best prarams:', clf.best_params_)\n",
        "\n",
        "svm_best_model = grid_search.best_estimator_\n",
        "y_pred = svm_best_model.predict(X_test)\n",
        "\n",
        "# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwI5EKyfpyX6",
        "outputId": "9a8a431c-b8ba-44ca-9899-c211b73ef430"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['save_model.joblib']"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save model\n",
        "# dump(svm_model, 'save_model.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "-RuqPKnqOVjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n vÃ´ hÃ¬nh"
      ],
      "metadata": {
        "id": "QWMYXH0oOdaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "S2oH3ZGhOc9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dá»± Ä‘oÃ¡n vÃ  Ä‘Ã¡nh giÃ¡"
      ],
      "metadata": {
        "id": "S3SBF7-SO1Mw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DQIjMv3pyX6",
        "outputId": "96d38851-55c2-4ffd-faa8-47f0fd7a1fe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.7334200260078023\n"
          ]
        }
      ],
      "source": [
        "# Dá»± Ä‘oÃ¡n nhÃ£n cho táº­p kiá»ƒm tra\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tinh chá»‰nh mÃ´ hÃ¬nh"
      ],
      "metadata": {
        "id": "xReHDcWUO3Zf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng2LVkGNpyX6",
        "outputId": "4965df1b-5e09-4e9c-f5a9-1a6dd2bcd3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Score: 0.7213650115962603\n",
            "F1 score: 0.7334200260078023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "15 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501\n",
            "        nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501\n",
            "        nan 0.72136501 0.72136501 0.72136501 0.72136501 0.72136501]\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression()\n",
        "\n",
        "# Thiáº¿t láº­p lÆ°á»›i tham sá»‘ cáº§n tÃ¬m kiáº¿m\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['lbfgs', 'liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# TÃ¬m kiáº¿m lÆ°á»›i cÃ¡c tham sá»‘ tá»‘t nháº¥t\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# In ra bá»™ tham sá»‘ tá»‘t nháº¥t vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n",
        "\n",
        "# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn dá»¯ liá»‡u kiá»ƒm tra\n",
        "y_pred = grid_search.predict(X_test)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"F1 score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Radom Forest"
      ],
      "metadata": {
        "id": "Bd-qqLEnO6wD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q1OuqN8pyX6",
        "outputId": "6fb0b47d-eb6d-4f7f-d27f-edb1a1be31d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters:  {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best score:  0.7213650115962603\n",
            "Accuracy: 0.7334200260078023\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Äá»‹nh nghÄ©a cÃ¡c tham sá»‘ cáº§n tinh chá»‰nh vÃ  giÃ¡ trá»‹ Ä‘á»ƒ thá»­ nghiá»‡m\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "\n",
        "# Táº¡o Ä‘á»‘i tÆ°á»£ng Grid Search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv = 5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Tiáº¿n hÃ nh Grid Search trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# In ra cÃ¡c thÃ´ng sá»‘ tá»‘t nháº¥t tá»« Grid Search\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n",
        "\n",
        "# Dá»± Ä‘oÃ¡n nhÃ£n cho dá»¯ liá»‡u kiá»ƒm tra sá»­ dá»¥ng mÃ´ hÃ¬nh tá»‘t nháº¥t\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c\n",
        "accuracy = f1_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGB"
      ],
      "metadata": {
        "id": "YZhiyzxePAqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tack31RpyX6",
        "outputId": "aa380b6d-816f-4ddb-fedf-d007b18ca404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8939929328621908\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "# Khá»Ÿi táº¡o mÃ´ hÃ¬nh XGBClassifier\n",
        "model = XGBClassifier()\n",
        "\n",
        "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn táº­p huáº¥n luyá»‡n\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Dá»± Ä‘oÃ¡n nhÃ£n cho táº­p kiá»ƒm tra\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh\n",
        "accuracy = f1_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdIUnTdhpyX6"
      },
      "outputs": [],
      "source": [
        "# model = XGBClassifier()\n",
        "\n",
        "# # Äá»‹nh nghÄ©a siÃªu tham sá»‘ vÃ  giÃ¡ trá»‹ Ä‘á»ƒ tinh chá»‰nh\n",
        "# param_grid = {\n",
        "#     'max_depth': [3, 5, 7],\n",
        "#     'learning_rate': [0.1, 0.01, 0.001],\n",
        "#     'n_estimators': [100, 500, 1000]\n",
        "# }\n",
        "\n",
        "# # Sá»­ dá»¥ng Grid Search Ä‘á»ƒ tÃ¬m siÃªu tham sá»‘ tá»‘t nháº¥t\n",
        "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='f1', cv=3, n_jobs=4)\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# # Láº¥y siÃªu tham sá»‘ tá»‘t nháº¥t vÃ  táº¡o mÃ´ hÃ¬nh vá»›i siÃªu tham sá»‘ Ä‘Ã³\n",
        "# best_params = grid_search.best_params_\n",
        "# model = XGBClassifier(**best_params)\n",
        "\n",
        "# # Huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn toÃ n bá»™ táº­p huáº¥n luyá»‡n\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# # Dá»± Ä‘oÃ¡n nhÃ£n cho táº­p kiá»ƒm tra\n",
        "# y_pred = model.predict(X_test)\n",
        "\n",
        "# # ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh\n",
        "# accuracy = f1_score(y_test, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}